import logging
import re
import base64
import time
from struct import pack
from datetime import datetime
from bson import ObjectId

from pymongo import MongoClient, TEXT
from pymongo.errors import DuplicateKeyError
from hydrogram.file_id import FileId

from info import (
    DATABASE_URL,
    DATABASE_NAME,
    MAX_BTN,
    USE_CAPTION_FILTER
)

logger = logging.getLogger(__name__)

# ======================================================
# üì¶ DATABASE CONNECTION (SINGLE DB)
# ======================================================

client = MongoClient(
    DATABASE_URL,
    maxPoolSize=50,
    minPoolSize=10,
    maxIdleTimeMS=45000
)

db = client[DATABASE_NAME]

primary_collection = db["primary_files"]
cloud_collection   = db["cloud_files"]
archive_collection = db["archive_files"]

COLLECTIONS = {
    "primary": primary_collection,
    "cloud": cloud_collection,
    "archive": archive_collection
}

# ======================================================
# ‚öôÔ∏è INDEXES (CRITICAL FOR SPEED)
# ======================================================

def create_indexes():
    for col in COLLECTIONS.values():
        # üî• TEXT INDEX (FAST SEARCH)
        col.create_index([("search_key", TEXT)])

        # üîí SAME FILE CAN EXIST IN MULTIPLE COLLECTIONS
        col.create_index(
            [("file_id", 1), ("collection_type", 1)],
            unique=True
        )

        # SUPPORTING INDEXES
        col.create_index("added_date")
        col.create_index("file_size")
        col.create_index("file_type")

    logger.info("‚úÖ MongoDB indexes ready (Ultra Fast)")

create_indexes()

# ======================================================
# ‚ö° SEARCH CACHE (RAM)
# ======================================================

SEARCH_CACHE = {}        # key -> (timestamp, data)
CACHE_TTL = 300          # 5 minutes

# ======================================================
# üß† HELPERS
# ======================================================

def normalize(text: str) -> str:
    text = text.lower()
    text = re.sub(r"@\w+", "", text)
    text = re.sub(r"[^a-z0-9 ]", " ", text)
    return re.sub(r"\s+", " ", text).strip()


def get_collection(name: str):
    return COLLECTIONS.get(name, primary_collection)

# ======================================================
# üíæ SAVE / INDEX FILE (ADMIN SELECTED COLLECTION)
# ======================================================

async def save_file(media, collection_name: str):
    """
    Index file into selected collection.
    Same file CAN exist in multiple collections.
    """

    file_id = unpack_new_file_id(media.file_id)

    file_name = media.file_name or ""
    caption = media.caption or ""

    # üî• WEIGHTED SEARCH KEY (file_name x2)
    if USE_CAPTION_FILTER:
        search_key = normalize(f"{file_name} {file_name} {caption}")
    else:
        search_key = normalize(f"{file_name} {file_name}")

    document = {
        "_id": ObjectId(),
        "file_id": file_id,
        "file_name": file_name,
        "caption": caption,
        "search_key": search_key,
        "file_size": media.file_size,
        "file_type": getattr(media, "mime_type", "unknown"),
        "collection_type": collection_name,
        "added_date": datetime.utcnow(),
        "downloads": 0,
        "last_accessed": datetime.utcnow()
    }

    col = get_collection(collection_name)

    try:
        col.insert_one(document)
        SEARCH_CACHE.clear()  # clear cache on new index
        logger.info(f"‚úÖ Indexed in {collection_name}: {file_name}")
        return "suc", collection_name

    except DuplicateKeyError:
        logger.warning(f"‚ö†Ô∏è Already indexed in {collection_name}: {file_name}")
        return "dup", collection_name

# ======================================================
# üîç ULTRA FAST SEARCH (TEXT + CACHE)
# ======================================================

async def get_search_results(query, collection_name=None, offset=0, limit=MAX_BTN):
    query = query.strip().lower()
    if not query:
        return [], "", 0, {}

    cache_key = f"{query}:{collection_name}:{offset}"
    now = time.time()

    # ‚ö° CACHE HIT
    if cache_key in SEARCH_CACHE:
        ts, data = SEARCH_CACHE[cache_key]
        if now - ts < CACHE_TTL:
            return data

    # üî• TOKENIZED TEXT SEARCH (AND LOGIC)
    tokens = query.split()
    text_query = {"$text": {"$search": " ".join(tokens)}}

    results = []
    counts = {}

    collections = (
        {collection_name: get_collection(collection_name)}
        if collection_name
        else COLLECTIONS
    )

    for name, col in collections.items():
        cursor = (
            col.find(
                text_query,
                {"score": {"$meta": "textScore"}}
            )
            .sort([("score", {"$meta": "textScore"})])
        )

        docs = list(cursor)
        for d in docs:
            d["source_collection"] = name

        counts[name] = len(docs)
        results.extend(docs)

    total = len(results)
    files = results[offset: offset + limit]

    next_offset = offset + limit
    if next_offset >= total:
        next_offset = ""

    data = (files, next_offset, total, counts)

    # üíæ SAVE TO CACHE
    SEARCH_CACHE[cache_key] = (now, data)

    return data

# ======================================================
# üî¢ SEARCH COUNTS ONLY (UI BUTTONS)
# ======================================================

async def get_search_counts(query):
    query = query.strip().lower()
    if not query:
        return {}

    q = {"$text": {"$search": query}}

    return {
        name: col.count_documents(q)
        for name, col in COLLECTIONS.items()
    }

# ======================================================
# üóëÔ∏è DELETE FILES
# ======================================================

async def delete_files(query, collection_name=None):
    query = query.strip().lower()
    if not query:
        return 0

    q = {"$text": {"$search": query}}
    deleted = 0

    if collection_name:
        deleted += get_collection(collection_name).delete_many(q).deleted_count
    else:
        for col in COLLECTIONS.values():
            deleted += col.delete_many(q).deleted_count

    SEARCH_CACHE.clear()
    return deleted

# ======================================================
# üìÇ FILE DETAILS
# ======================================================

async def get_file_details(file_id):
    for col in COLLECTIONS.values():
        doc = col.find_one({"file_id": file_id})
        if doc:
            return doc
    return None

# ======================================================
# üîÅ MOVE / COPY (MANUAL ONLY)
# ======================================================

async def move_file(file_id, from_col, to_col):
    src = get_collection(from_col)
    dst = get_collection(to_col)

    file = src.find_one({"file_id": file_id})
    if not file:
        return False, "File not found"

    file["_id"] = ObjectId()
    file["collection_type"] = to_col
    file["moved_date"] = datetime.utcnow()

    try:
        dst.insert_one(file)
        src.delete_one({"file_id": file_id})
        SEARCH_CACHE.clear()
        return True, "Moved successfully"
    except DuplicateKeyError:
        return False, "Already exists in target"


async def copy_file(file_id, from_col, to_col):
    src = get_collection(from_col)
    dst = get_collection(to_col)

    file = src.find_one({"file_id": file_id})
    if not file:
        return False, "File not found"

    file["_id"] = ObjectId()
    file["collection_type"] = to_col
    file["copied_date"] = datetime.utcnow()

    try:
        dst.insert_one(file)
        SEARCH_CACHE.clear()
        return True, "Copied successfully"
    except DuplicateKeyError:
        return False, "Already exists in target"

# ======================================================
# üß¨ FILE ID ENCODING (TELEGRAM SAFE)
# ======================================================

def encode_file_id(s: bytes) -> str:
    r = b""
    n = 0
    for i in s + bytes([22]) + bytes([4]):
        if i == 0:
            n += 1
        else:
            if n:
                r += b"\x00" + bytes([n])
                n = 0
            r += bytes([i])
    return base64.urlsafe_b64encode(r).decode().rstrip("=")


def unpack_new_file_id(new_file_id):
    decoded = FileId.decode(new_file_id)
    return encode_file_id(
        pack(
            "<iiqq",
            int(decoded.file_type),
            decoded.dc_id,
            decoded.media_id,
            decoded.access_hash
        )
    )
